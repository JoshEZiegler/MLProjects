2018-12-4:
    The simple linear model doesn't seem to do very well. At first, the best I could do was 54% accuracy - less than if it simply guessed that radiant won every time. I think this makes sense because the features should be highly correlated - no hero alone can predict whether a team will win. I might try some feature engineering but it may work best to move to a DNN model.
    A DNN model seems more natural because heroes belong better in an embedding column (which is a trained layer in a neural network) since there are 116 heroes, but there are simple, lower-dimensional groupings that can be made.
    Before doing this I'm going to try putting all of the heroes into a single feature column and see if that helps. It should?

2018-12-8:
    I was able to get accuracy=58.6 on the test set (accuracy_baseline=55.5%) for a linear_classifier model. Parameters used were: learning_rate=0.03, steps=10000, batch_size=1000. These are a bit aggressive and the steps could probably have been reduced by a factor of ten. In any case, I don’t think that the linear model is going to do any better than this. 
	Across the training period, it didn’t look like the accuracy for the training set decreased much past  60%, but the test set inaccuracy dropped about 1% from ~57.3% to 58.2%. So the extra training time helped the model generalize a bit even if it wasn’t evident from the training accuracy.
	So it looks like the DNN with an indicator_column (one-hot encoding to made a dense vector) is pretty hard to train with the FtrlOptimizer. If the parameters are slightly off the model is prone to either serious overfitting of the training data or underfitting. (hidden_units=[20,10,5])
	Just tried the AdamOptimizer and it worked as good as the FtrlOptimizer with default parameters learning_rate=0.01, hidden_units=[20,10,5]. Not sure why? Here’s some reading on the algorithm (https://arxiv.org/pdf/1412.6980.pdf). It says it’s a gradient descent optimization of stochastic objective functions. Also, “The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.” This algorithm also has a learning rate decay which probs helps reduce noise at late steps.
	In order to do regularization, AdamWOptimizer is nice. It is the same algorithm with a weight decay that regularizes. “It computes the update step of train.AdamOptimizer and additionally decays the variable. Note that this is different from adding L2 regularization on the variables to the loss: it regularizes variables with large gradients more than L2 regularization would, which was shown to yield better training loss and generalization error in the paper above.” I got 58% accuracy with weight_decay=0.001, learning_rate=0.01, hidden_units=[20,10,10,5], steps=10000, batch_size=1000. I’m not sure If I can do better without an embedding layer.
	I was able to get ~58% accuracy with hidden_units=[4,4] which seemed a little better at avoiding overfitting issues. Also weight_decay=0.002, learning_rate=0.01, steps=500, batch_size=1000.Still this isn’t matching the linear model…

2018-12-9:
	Using embedding columns with dimension=2 for the hero inputs easily matches the linear model’s performance. I’m going to look at what the embedding layer is doing to see if it is categorizing heroes in an intuitive way. Then I’ll play around with the hyperparameters and embedding layer dimension. I’d expect that the embedding layer would be better because I can definitely think of >2 independent ways to classify heroes.
	The DNN classifier with an Embedding column, d=4 not really better than d=2, let alone the linear classifier. By plotting the different parts of the embedding layer it looks like the 4th dimension didn’t add much because slices (plotting 4 vs 0 or 2) look like lines, but I also couldn’t really tell the difference between the other ones.
	This project might be a wash because it doesn’t look like the model can do very well at all regardless of complexity. Maybe this is because the data isn’t diverse enough and I should include lower skill level matches. In any case, I want to find a way to get the confusion matrix out as well as look at what the loss is doing as the accuracy gets smaller. If the loss is doing weird stuff it might be a good option to make my own estimator?

2019-2-2:
	After the poor results of the DNN and linear model I got a bit discouraged and just took a bunch of data at various skill levels. I got a total of 7e5 dota2 matches.   This may have reduced the amount of noise while learning? It looks like the training and test set accuracy match v well all the way down to the maximum accuracy. I still was only able to get 58.62% accuracy (compared to a 54.52 baseline accuracy) with the linear model. The agreement between test and training data as well as reproducibility is encouraging. I was v conservative with learning rate and had to do a bunch of steps, but I liked the results. learning_rate=0.002, steps=1e6, batch_size=5000
 
	Linear model, best case Metrics on test set:
		accuracy 0.5862,			
		accuracy_baseline 0.5452214, 
		auc 0.6078432,
		auc_precision_recall 0.6372294, 
		average_loss 0.67102975, 
		label/mean 0.5452214, 
		loss 33.55103, 
		precision 0.59496564, 
		prediction/mean 0.54470664, 
		recall 0.75519633
		global_step 1000000
	Metrics on validation set:
		accuracy 0.5852
		accuracy_baseline 0.5486
		auc 0.6071559
		auc_precision_recall 0.6377059
		average_loss 0.67106634
		label/mean 0.5486
		loss 33.553318
		precision 0.59609306
		prediction/mean 0.5435795
		recall 0.75647104
		global_step 1000000

	My hope is that with this larger amount of data the neural network may be able to find the nonlinear terms more easily. We’ll see if this turns out to be the case!

2019-2-3:	
	LOL TESTS: I used the linear model on the League of Legends data and the accuracy gain over the baseline accuracy was the same as dota2. Baseline: 50.36%, Accuracy: 54.68% with 2e5 games. The model was a bit worse on validation data, but that could just be because it was noisy, baseline accuracy was off by 0.7% compared to the test set. It looked like with more steps it was possible to push it a few hundredths of a percent higher, but this took a couple hours so its fine enough. learning_rate=0.003, steps=5e5, batch_size=2000

	Linear model, best case metrics on test set:
		accuracy 0.54675937
		accuracy_baseline 0.5036985
		auc 0.5667639
		auc_precision_recall 0.56329566
		average_loss 0.6861083
		label/mean 0.5036985
		loss 34.29707
		precision 0.5493952
		prediction/mean 0.5016105
		recall 0.55709404
		global_step 500000
	Validation set:
		accuracy 0.543
		accuracy_baseline 0.5102
		auc 0.56560886
		auc_precision_recall 0.5488617
		average_loss 0.68615097
		label/mean 0.4898
		loss 34.30755
		precision 0.5322327
		prediction/mean 0.50139517
		recall 0.55287874
		global_step 500000
