{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T05:16:03.602299Z",
     "start_time": "2018-12-06T05:16:03.597032Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:05:21.377523Z",
     "start_time": "2018-12-06T07:05:21.290323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>radiant_win</th>\n",
       "      <th>radiant_heroes0</th>\n",
       "      <th>dire_heroes0</th>\n",
       "      <th>radiant_heroes1</th>\n",
       "      <th>dire_heroes1</th>\n",
       "      <th>radiant_heroes2</th>\n",
       "      <th>dire_heroes2</th>\n",
       "      <th>radiant_heroes3</th>\n",
       "      <th>dire_heroes3</th>\n",
       "      <th>radiant_heroes4</th>\n",
       "      <th>dire_heroes4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.797000e+03</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "      <td>8797.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.253016e+09</td>\n",
       "      <td>0.549733</td>\n",
       "      <td>50.808116</td>\n",
       "      <td>51.427759</td>\n",
       "      <td>52.109924</td>\n",
       "      <td>51.585313</td>\n",
       "      <td>51.716153</td>\n",
       "      <td>51.900307</td>\n",
       "      <td>51.776401</td>\n",
       "      <td>51.577924</td>\n",
       "      <td>50.882346</td>\n",
       "      <td>51.083665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.790517e+06</td>\n",
       "      <td>0.497549</td>\n",
       "      <td>34.429014</td>\n",
       "      <td>34.492457</td>\n",
       "      <td>34.563422</td>\n",
       "      <td>34.404611</td>\n",
       "      <td>34.813327</td>\n",
       "      <td>34.561568</td>\n",
       "      <td>34.140065</td>\n",
       "      <td>34.651073</td>\n",
       "      <td>34.428431</td>\n",
       "      <td>34.512934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.252092e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.252121e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.252128e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.252140e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.256625e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           match_id  radiant_win  radiant_heroes0  dire_heroes0  \\\n",
       "count  8.797000e+03  8797.000000      8797.000000   8797.000000   \n",
       "mean   4.253016e+09     0.549733        50.808116     51.427759   \n",
       "std    1.790517e+06     0.497549        34.429014     34.492457   \n",
       "min    4.252092e+09     0.000000         1.000000      1.000000   \n",
       "25%    4.252121e+09     0.000000        21.000000     21.000000   \n",
       "50%    4.252128e+09     1.000000        44.000000     45.000000   \n",
       "75%    4.252140e+09     1.000000        81.000000     82.000000   \n",
       "max    4.256625e+09     1.000000       121.000000    121.000000   \n",
       "\n",
       "       radiant_heroes1  dire_heroes1  radiant_heroes2  dire_heroes2  \\\n",
       "count      8797.000000   8797.000000      8797.000000   8797.000000   \n",
       "mean         52.109924     51.585313        51.716153     51.900307   \n",
       "std          34.563422     34.404611        34.813327     34.561568   \n",
       "min           1.000000      1.000000         1.000000      1.000000   \n",
       "25%          22.000000     22.000000        21.000000     22.000000   \n",
       "50%          46.000000     44.000000        45.000000     47.000000   \n",
       "75%          84.000000     83.000000        84.000000     84.000000   \n",
       "max         121.000000    121.000000       121.000000    121.000000   \n",
       "\n",
       "       radiant_heroes3  dire_heroes3  radiant_heroes4  dire_heroes4  \n",
       "count      8797.000000   8797.000000      8797.000000   8797.000000  \n",
       "mean         51.776401     51.577924        50.882346     51.083665  \n",
       "std          34.140065     34.651073        34.428431     34.512934  \n",
       "min           1.000000      1.000000         1.000000      1.000000  \n",
       "25%          22.000000     22.000000        21.000000     21.000000  \n",
       "50%          46.000000     45.000000        44.000000     44.000000  \n",
       "75%          82.000000     84.000000        81.000000     82.000000  \n",
       "max         121.000000    121.000000       121.000000    121.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dota2_df = None\n",
    "for filename in glob.glob(os.path.join('dota2_training_data','*.csv')):\n",
    "    if type(dota2_df)==None:\n",
    "        dota2_df = pd.read_csv(filename, usecols=range(1,13))\n",
    "    else:\n",
    "        dota2_df = pd.concat([dota2_df, pd.read_csv(filename, usecols=range(1,13))],ignore_index=True)\n",
    "\n",
    "dota2_df = dota2_df.reindex(np.random.permutation(dota2_df.index))\n",
    "dota2_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:05:26.416329Z",
     "start_time": "2018-12-06T07:05:26.399107Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_features(dota2_df):\n",
    "    \"\"\"Take dota2_df and create a dataframe containing only the features for our model\n",
    "            Args: \n",
    "                dota2_df: Dataframe containing dota2 training and test data\n",
    "            returns: \n",
    "                processed_df: pandas DataFrame containing only feature columns\n",
    "    \"\"\"\n",
    "    \n",
    "    #Use arrays of heroes for each team as features rather than each hero\n",
    "    #individually to help model fit?\n",
    "    #This is an attempt to make the model more similar to the movie review\n",
    "    #text analysis example in the Google ML Crash Course\n",
    "    processed_df = pd.DataFrame()\n",
    "    \n",
    "    processed_df['radiant_heroes'] = list(np.array(dota2_df.loc[:,['radiant_heroes0',\n",
    "                                                'radiant_heroes1','radiant_heroes2',\n",
    "                                                'radiant_heroes3','radiant_heroes4']]))\n",
    "    processed_df['dire_heroes'] = list(np.array(dota2_df.loc[:,['dire_heroes0',\n",
    "                                    'dire_heroes1','dire_heroes2',\n",
    "                                    'dire_heroes3','dire_heroes4']]))\n",
    "    \n",
    "    ##only features (to start with) are the heroes in the game\n",
    "    #processed_df = dota2_df[['radiant_heroes0', 'radiant_heroes1',\n",
    "    #                   'radiant_heroes2', 'radiant_heroes3', 'radiant_heroes4',\n",
    "    #                    'dire_heroes0', 'dire_heroes1', 'dire_heroes2',\n",
    "    #                    'dire_heroes3', 'dire_heroes4']]\n",
    "    \n",
    "    ##create two synthetic features that is the product of all heroes for each team\n",
    "    #processed_df['radiant_hero_product'] = dota2_df['radiant_heroes0']*dota2_df['radiant_heroes1']*dota2_df['radiant_heroes2']dota2_df['radiant_heroes3']*dota2_df['radiant_heroes4']\n",
    "    #processed_df['dire_hero_product'] = dota2_df['dire_heroes0']*dota2_df['dire_heroes1']*dota2_df['dire_heroes2']dota2_df['dire_heroes3']*dota2_df['dire_heroes4']\n",
    "    \n",
    "    return processed_df\n",
    "    \n",
    "def preprocess_targets(dota2_df):\n",
    "    \"\"\"Take dota2_df and create a dataframe containing only the targets for our model\n",
    "            Args: \n",
    "                dota2_df: Dataframe containing dota2 training and test data\n",
    "            returns: \n",
    "                target_df: pandas DataFrame containing only the target column\n",
    "    \"\"\"\n",
    "    target_df = pd.DataFrame()\n",
    "    target_df['radiant_win'] = dota2_df['radiant_win']\n",
    "    \n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:06:11.844506Z",
     "start_time": "2018-12-06T07:06:11.781414Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training targets summary\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radiant_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.549943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.497540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       radiant_win\n",
       "count  6157.000000\n",
       "mean      0.549943\n",
       "std       0.497540\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation target summary\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radiant_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.549242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.497664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       radiant_win\n",
       "count  2640.000000\n",
       "mean      0.549242\n",
       "std       0.497664\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Split training and validation datasets with a given training fraction. \n",
    "\n",
    "#to begin with we have a sample of 7050 matches to validation the model with.\n",
    "#This may not be enough games to accurately determine win probability but probably good enough for first validations\n",
    "\n",
    "training_fraction = 0.7\n",
    "head_num = int(training_fraction * len(dota2_df))\n",
    "tail_num = len(dota2_df)-head_num\n",
    "\n",
    "training_examples = preprocess_features(dota2_df.head(head_num))\n",
    "training_targets = preprocess_targets(dota2_df.head(head_num))\n",
    "\n",
    "validation_examples = preprocess_features(dota2_df.tail(tail_num))\n",
    "validation_targets = preprocess_targets(dota2_df.tail(tail_num))\n",
    "\n",
    "#print(\"Training features summary\")\n",
    "#training_examples.style\n",
    "print(\"\\nTraining targets summary\")\n",
    "display.display(training_targets.describe())\n",
    "\n",
    "#print(\"\\nvalidation feature summary\")\n",
    "#validation_examples.style\n",
    "print(\"\\nvalidation target summary\")\n",
    "display.display(validation_targets.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TFRecords file\n",
    "Do this to nicely handle the features that is a numpy array (radiant_heroes and dire_heroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:06:30.173428Z",
     "start_time": "2018-12-06T07:06:30.168196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:06:32.673475Z",
     "start_time": "2018-12-06T07:06:32.649771Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_tfrecords(filename, examples, targets):\n",
    "    # open the TFRecords file\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    \n",
    "    radiant_hero_array = np.array(examples['radiant_heroes'])\n",
    "    dire_hero_array = np.array(examples['dire_heroes'])\n",
    "    target_array = np.array(targets['radiant_win'])\n",
    "    \n",
    "    for i in range(len(radiant_hero_array[:])):\n",
    "        # print how many images are saved every 1000 images\n",
    "        if not i % 1000:\n",
    "            print('Train data: %d/%d' % (i, len(examples)))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # Load the image\n",
    "        radiant_heroes = radiant_hero_array[:][i].tostring()\n",
    "        dire_heroes = dire_hero_array[:][i].tostring()\n",
    "        target = target_array[i]\n",
    "        \n",
    "        # Create a feature\n",
    "        feature = {'train/radiant_heroes': _bytes_feature(tf.compat.as_bytes(radiant_heroes)),\n",
    "                   'train/dire_heroes': _bytes_feature(tf.compat.as_bytes(dire_heroes)),\n",
    "                   'train/targets': _int64_feature(target)}\n",
    "        # Create an example protocol buffer\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "        # Serialize to string and write on the file\n",
    "        writer.write(example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:10:38.135976Z",
     "start_time": "2018-12-06T07:10:37.193730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 0/8797\n",
      "Train data: 1000/8797\n",
      "Train data: 2000/8797\n",
      "Train data: 3000/8797\n",
      "Train data: 4000/8797\n",
      "Train data: 5000/8797\n",
      "Train data: 6000/8797\n",
      "Train data: 7000/8797\n",
      "Train data: 8000/8797\n"
     ]
    }
   ],
   "source": [
    "convert_to_tfrecords(os.path.join('dota2_training_data', 'dota2_training_data.tfrecords'),\n",
    "                     preprocess_features(dota2_df),\n",
    "                     preprocess_targets(dota2_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Build input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T06:17:03.194962Z",
     "start_time": "2018-12-06T06:17:03.185169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _parse_function(record):\n",
    "    \"\"\"Extracts features and labels.\n",
    "  \n",
    "    Args:\n",
    "        record: File path to a TFRecord file    \n",
    "      Returns:\n",
    "    A `tuple` `(labels, features)`:\n",
    "      features: A dict of tensors representing the features\n",
    "      labels: A tensor with the corresponding labels.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"train/radiant_heroes\": tf.FixedLenFeature([], dtype=tf.string), #array of five 8 bit ints for heroes\n",
    "        \"train/dire_heroes\": tf.FixedLenFeature([], dtype=tf.string), #array of five 8-bit ints for heroes\n",
    "        \"train/targets\": tf.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "                }\n",
    "  \n",
    "    parsed_features = tf.parse_single_example(record, features)\n",
    "  \n",
    "    radiant_heroes = tf.decode_raw(parsed_features['train/radiant_heroes'], tf.int64)\n",
    "    dire_heroes = tf.decode_raw(parsed_features['train/dire_heroes'], tf.int64)\n",
    "    radiant_win = parsed_features['train/targets']\n",
    "\n",
    "    return  {'radiant_heroes':radiant_heroes, 'dire_heroes':dire_heroes}, radiant_win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Check the parse function worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T06:17:05.797812Z",
     "start_time": "2018-12-06T06:17:05.776845Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ({radiant_heroes: (?,), dire_heroes: (?,)}, (1,)), types: ({radiant_heroes: tf.int64, dire_heroes: tf.int64}, tf.int64)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Dataset object.\n",
    "ds = tf.data.TFRecordDataset(os.path.join('dota2_training_data','match_ids_4252145739-4252126461.tfrecords'))\n",
    "# Map features and labels with the parse function.\n",
    "ds = ds.map(_parse_function)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T06:17:10.475254Z",
     "start_time": "2018-12-06T06:17:10.434800Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dire_heroes': array([33, 97, 11,  6,  2]),\n",
       "  'radiant_heroes': array([ 36,  41,  69, 105,  10])},\n",
       " array([0]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = ds.make_one_shot_iterator().get_next()\n",
    "sess = tf.Session()\n",
    "sess.run(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Build Functions for doing linear classifier modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:52:07.895571Z",
     "start_time": "2018-12-05T05:52:07.889479Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def construct_hero_categorical_columns(input_features):\n",
    "    \"\"\"Construct categorical features for all features\n",
    "        Args: \n",
    "            input_features: names of input feature columns to use\n",
    "        returns:\n",
    "            a set of feature columns\n",
    "    \"\"\"\n",
    "    vocab = list(range(105))+list(range(106,114))+list(range(119,121))\n",
    "    return set([tf.feature_column.categorical_column_with_vocabulary_list(my_feature,\n",
    "                                                                          vocabulary_list = vocab, num_oov_buckets =1)\n",
    "                                                                for my_feature in input_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T05:52:09.516609Z",
     "start_time": "2018-12-05T05:52:09.505248Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a logistic regression model.\n",
    "  \n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
    " \n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_linear_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    feature_columns,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "    \"\"\"Trains a linear regression model.\n",
    "  \n",
    "    In addition to training, this function also prints training progress information,\n",
    "    as well as a plot of the training and validation loss over time.\n",
    "      \n",
    "    Args:\n",
    "        learning_rate: A `float`, the learning rate.\n",
    "        steps: A non-zero `int`, the total number of training steps. A training step\n",
    "          consists of a forward and backward pass using a single batch.\n",
    "        feature_columns: A `set` specifying the input feature columns to use.\n",
    "        training_examples: A `DataFrame` containing one or more columns from\n",
    "          `dota2_df` to use as input features for training.\n",
    "        training_targets: A `DataFrame` containing exactly one column from\n",
    "          `dota2_df` to use as target for training.\n",
    "      \n",
    "    Returns:\n",
    "        A `LinearClassifier` object trained on the training data.\n",
    "    \"\"\"\n",
    "\n",
    "    periods = 20\n",
    "    steps_per_period = steps / periods\n",
    "\n",
    "    # Create a linear regressor object.\n",
    "    my_optimizer = tf.train.FtrlOptimizer(learning_rate=learning_rate)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "    linear_classifier = tf.estimator.LinearClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      optimizer=my_optimizer,\n",
    "      config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
    "      )\n",
    "  \n",
    "    training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets[\"radiant_win\"], \n",
    "                                          batch_size=batch_size)\n",
    "    predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets[\"radiant_win\"], \n",
    "                                          num_epochs=1, \n",
    "                                          shuffle=False)\n",
    "    \n",
    "    validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                          validation_targets[\"radiant_win\"], \n",
    "                                          batch_size=batch_size)\n",
    "    predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                          validation_targets[\"radiant_win\"], \n",
    "                                          num_epochs=1, \n",
    "                                          shuffle=False)\n",
    "    # Train the model, but do so inside a loop so that we can periodically assess\n",
    "    # loss metrics.\n",
    "    print(\"Training model...\")\n",
    "    print(\"LogLoss error (on validation data):\")\n",
    "\n",
    "    training_errors = []\n",
    "    validation_errors = []\n",
    "    for period in range (0, periods):\n",
    "        # Train the model, starting from the prior state.\n",
    "        linear_classifier.train(\n",
    "            input_fn=training_input_fn,\n",
    "            steps=steps_per_period\n",
    "            )\n",
    "        # Take a break and compute predictions.\n",
    "        training_predictions = list(linear_classifier.predict(input_fn=predict_training_input_fn))\n",
    "        training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
    "        training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
    "        training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,2)\n",
    "            \n",
    "        validation_predictions = list(linear_classifier.predict(input_fn=predict_validation_input_fn))\n",
    "        validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
    "        validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
    "        validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,2)    \n",
    "        print(\"Model training finished.\")\n",
    "        # Compute training and validation errors.\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
    "        validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
    "        # Occasionally print the current loss.\n",
    "        print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
    "        # Add the loss metrics from this period to our list.\n",
    "        training_errors.append(training_log_loss)\n",
    "        validation_errors.append(validation_log_loss)\n",
    "    print(\"Model training finished.\")\n",
    "    \n",
    "    # Remove event files to save disk space.\n",
    "    _ = map(os.remove, glob.glob(os.path.join(linear_classifier.model_dir, 'events.out.tfevents*')))\n",
    "  \n",
    "    # Calculate final predictions (not probabilities, as above).\n",
    "    final_predictions = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "    final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
    "  \n",
    "  \n",
    "    accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "    print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
    "\n",
    "    # Output a graph of loss metrics over periods.\n",
    "    plt.ylabel(\"LogLoss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"LogLoss vs. Periods\")\n",
    "    plt.plot(training_errors, label=\"training\")\n",
    "    plt.plot(validation_errors, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "  \n",
    "    # Output a plot of the confusion matrix.\n",
    "    cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "    # in each class).\n",
    "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "    ax.set_aspect(1)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "    return linear_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/2_/ffhj27wn1bbgt7ldy1qll5600000gn/T/tmpjm0y4te1\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/2_/ffhj27wn1bbgt7ldy1qll5600000gn/T/tmpjm0y4te1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x16fd17f28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "Training model...\n",
      "LogLoss error (on validation data):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got array([33, 97, 11,  6,  2])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-3753d4067486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtraining_targets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     validation_targets=validation_targets)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-8927182acb5f>\u001b[0m in \u001b[0;36mtrain_linear_model\u001b[0;34m(learning_rate, steps, batch_size, feature_columns, training_examples, training_targets, validation_examples, validation_targets)\u001b[0m\n\u001b[1;32m     65\u001b[0m         linear_classifier.train(\n\u001b[1;32m     66\u001b[0m             \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_period\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Take a break and compute predictions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1232\u001b[0m       features, labels, input_hooks = (\n\u001b[1;32m   1233\u001b[0m           self._get_features_and_labels_from_input_fn(\n\u001b[0;32m-> 1234\u001b[0;31m               input_fn, model_fn_lib.ModeKeys.TRAIN))\n\u001b[0m\u001b[1;32m   1235\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m       estimator_spec = self._call_model_fn(\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     return estimator_util.parse_input_fn_result(\n\u001b[0;32m-> 1075\u001b[0;31m         self._call_input_fn(input_fn, mode))\n\u001b[0m\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extract_batch_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m   1160\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-8927182acb5f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     training_input_fn = lambda: my_input_fn(training_examples, \n\u001b[1;32m     42\u001b[0m                                           \u001b[0mtraining_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"radiant_win\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                           batch_size=batch_size)\n\u001b[0m\u001b[1;32m     44\u001b[0m     predict_training_input_fn = lambda: my_input_fn(training_examples, \n\u001b[1;32m     45\u001b[0m                                           \u001b[0mtraining_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"radiant_win\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-00fc82d69e58>\u001b[0m in \u001b[0;36mmy_input_fn\u001b[0;34m(features, targets, batch_size, shuffle, num_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Construct a dataset, and configure batching/repeating.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# warning: 2GB limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m   1563\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[1;32m   1564\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[0;32m-> 1565\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m       ])\n\u001b[1;32m   1567\u001b[0m       \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1563\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[1;32m   1564\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[0;32m-> 1565\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m       ])\n\u001b[1;32m   1567\u001b[0m       \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                          as_ref=False):\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    540\u001b[0m     raise TypeError(\n\u001b[1;32m    541\u001b[0m         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\n\u001b[0;32m--> 542\u001b[0;31m   \u001b[0mappend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtensorflow/python/framework/fast_tensor_util.pyx\u001b[0m in \u001b[0;36mtensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/bailey/anaconda/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 61\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got array([33, 97, 11,  6,  2])"
     ]
    }
   ],
   "source": [
    "trained_linear_model = train_linear_model(\n",
    "    learning_rate=0.01,\n",
    "    steps=500,\n",
    "    batch_size=50,\n",
    "    feature_columns=construct_hero_categorical_columns(training_examples),\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_dnn_model(\n",
    "    learning_rate,\n",
    "    regularization,\n",
    "    hidden_units,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    feature_columns,\n",
    "    training_examples,\n",
    "    training_targets):\n",
    "    \"\"\"Trains a deep neural network regression model.\n",
    "  \n",
    "    In addition to training, this function also prints training progress information,\n",
    "    as well as a plot of the training and validation loss over time.\n",
    "      \n",
    "    Args:\n",
    "        learning_rate: A `float`, the learning rate.\n",
    "        l1_regularization: A `float`, the L1 regularization rate. This regularizatin \n",
    "          penalizes non-zero weights in the model to prevent overfitting as well as\n",
    "          save memory.\n",
    "        l2_regularization: A `float`, the L2 regularization rate. This regularization\n",
    "          penalizes large weights in the model to prevent overfitting.\n",
    "        hidden_units: A list of `ints`, the number of nodes in each layer for the DNN\n",
    "          model.\n",
    "        steps: A non-zero `int`, the total number of training steps. A training step\n",
    "          consists of a forward and backward pass using a single batch.\n",
    "        feature_columns: A `set` specifying the input feature columns to use.\n",
    "        training_examples: A `DataFrame` containing one or more columns from\n",
    "          `dota2_df` to use as input features for training.\n",
    "        training_targets: A `DataFrame` containing exactly one column from\n",
    "          `dota2_df` to use as target for training.\n",
    "      \n",
    "    Returns:\n",
    "        A `DNNClassifer` object trained on the training data.\n",
    "    \"\"\"\n",
    "\n",
    "    periods = 10\n",
    "    steps_per_period = steps / periods\n",
    "\n",
    "    # Create a linear regressor object.\n",
    "    my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "    dnn_classifier = tf.estimator.DNNClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      hidden_units=hidden_units,\n",
    "      optimizer=my_optimizer\n",
    "      )\n",
    "  \n",
    "    training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets[\"radiant_win\"], \n",
    "                                          batch_size=batch_size)\n",
    "    predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                  training_targets[\"radiant_win\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "\n",
    "    # Train the model, but do so inside a loop so that we can periodically assess\n",
    "    # loss metrics.\n",
    "    print(\"Training model...\")\n",
    "    print(\"RMSE (on training data):\")\n",
    "    training_rmse = []\n",
    "    for period in range (0, periods):\n",
    "        # Train the model, starting from the prior state.\n",
    "        dnn_classifier.train(\n",
    "            input_fn=training_input_fn,\n",
    "            steps=steps_per_period\n",
    "            )\n",
    "        # Take a break and compute predictions.\n",
    "        training_predictions = dnn_classifier.predict(input_fn=predict_training_input_fn)\n",
    "    \n",
    "    print(\"Model training finished.\")\n",
    "\n",
    "\n",
    "    return dnn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trained_linear_model = train_dnn_model(\n",
    "    learning_rate=0.07,\n",
    "    regularization=0.01\n",
    "    steps=1000,\n",
    "    batch_size=100,\n",
    "    feature_columns=construct_hero_categorical_columns(training_examples),\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
