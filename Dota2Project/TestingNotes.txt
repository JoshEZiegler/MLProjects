2018-12-4:
    The simple linear model doesn't seem to do very well. At first, the best I could do was 54% accuracy - less than if it simply guessed that radiant won every time. I think this makes sense because the features should be highly correlated - no hero alone can predict whether a team will win. I might try some feature engineering but it may work best to move to a DNN model.
    A DNN model seems more natural because heroes belong better in an embedding column (which is a trained layer in a neural network) since there are 116 heroes, but there are simple, lower-dimensional groupings that can be made.
    Before doing this I'm going to try putting all of the heroes into a single feature column and see if that helps. It should?

2018-12-8:
    I was able to get accuracy=58.6 on the test set (accuracy_baseline=55.5%) for a linear_classifier model. Parameters used were: learning_rate=0.03, steps=10000, batch_size=1000. These are a bit aggressive and the steps could probably have been reduced by a factor of ten. In any case, I don’t think that the linear model is going to do any better than this. 
	Across the training period, it didn’t look like the accuracy for the training set decreased much past  60%, but the test set inaccuracy dropped about 1% from ~57.3% to 58.2%. So the extra training time helped the model generalize a bit even if it wasn’t evident from the training accuracy.
	So it looks like the DNN with an indicator_column (one-hot encoding to made a dense vector) is pretty hard to train with the FtrlOptimizer. If the parameters are slightly off the model is prone to either serious overfitting of the training data or underfitting. (hidden_units=[20,10,5])
	Just tried the AdamOptimizer and it worked as good as the FtrlOptimizer with default parameters learning_rate=0.01, hidden_units=[20,10,5]. Not sure why? Here’s some reading on the algorithm (https://arxiv.org/pdf/1412.6980.pdf). It says it’s a gradient descent optimization of stochastic objective functions. Also, “The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.” This algorithm also has a learning rate decay which probs helps reduce noise at late steps.
	In order to do regularization, AdamWOptimizer is nice. It is the same algorithm with a weight decay that regularizes. “It computes the update step of train.AdamOptimizer and additionally decays the variable. Note that this is different from adding L2 regularization on the variables to the loss: it regularizes variables with large gradients more than L2 regularization would, which was shown to yield better training loss and generalization error in the paper above.” I got 58% accuracy with weight_decay=0.001, learning_rate=0.01, hidden_units=[20,10,10,5], steps=10000, batch_size=1000. I’m not sure If I can do better without an embedding layer.
	I was able to get ~58% accuracy with hidden_units=[4,4] which seemed a little better at avoiding overfitting issues. Also weight_decay=0.002, learning_rate=0.01, steps=500, batch_size=1000.Still this isn’t matching the linear model…

2018-12-9:
	Using embedding columns with dimension=2 for the hero inputs easily matches the linear model’s performance. I’m going to look at what the embedding layer is doing to see if it is categorizing heroes in an intuitive way. Then I’ll play around with the hyperparameters and embedding layer dimension. I’d expect that the embedding layer would be better because I can definitely think of >2 independent ways to classify heroes.
	The DNN classifier with an Embedding column, d=4 not really better than d=2, let alone the linear classifier. By plotting the different parts of the embedding layer it looks like the 4th dimension didn’t add much because slices (plotting 4 vs 0 or 2) look like lines, but I also couldn’t really tell the difference between the other ones.
	This project might be a wash because it doesn’t look like the model can do very well at all regardless of complexity. Maybe this is because the data isn’t diverse enough and I should include lower skill level matches. In any case, I want to find a way to get the confusion matrix out as well as look at what the loss is doing as the accuracy gets smaller. If the loss is doing weird stuff it might be a good option to make my own estimator?
