{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T16:59:09.564552Z",
     "start_time": "2019-08-04T16:59:07.881392Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T16:59:09.575301Z",
     "start_time": "2019-08-04T16:59:09.566963Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T16:59:10.738326Z",
     "start_time": "2019-08-04T16:59:09.577795Z"
    }
   },
   "outputs": [],
   "source": [
    "html_raw = simple_get('https://pubs.acs.org/doi/10.1021/acs.nanolett.9b00357')\n",
    "print(len(html_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:35:45.032119Z",
     "start_time": "2019-08-04T17:35:45.023919Z"
    }
   },
   "outputs": [],
   "source": [
    "#class to contain the tags and names of various parts of the retrieved paper\n",
    "#tags are specfific to a journal or family of journals, so the initilization\n",
    "#    of the object must specify the family or it will return nothing\n",
    "##\n",
    "#class journal has attributes: abstract, body, and references\n",
    "#these attributes are all dicts specifying the html tag and name of the\n",
    "#part of the html code to be retrieved\n",
    "class journal_format:\n",
    "    \n",
    "    def __init__(self, family):\n",
    "        #use series of if statements to specify the attributes of the\n",
    "        #journal object\n",
    "        if family.lower() == 'acs':\n",
    "            self.abstract = {'tag': 'p', 'name': 'articleBody_abstractText'}\n",
    "            self.body = {'tag': 'div', 'name': 'NLM_p'}\n",
    "            self.figure_captions = {'tag': 'p', 'name': 'first last'}\n",
    "            self.reference_title = {'tag': 'span', 'name': 'NLM_cas:atitle'}\n",
    "            self.reference_abstract = {'tag': 'div', 'name': 'casAbstract'}\n",
    "        else:\n",
    "            self.abstract = None\n",
    "            self.body = None\n",
    "            self.figure_captions = None\n",
    "            self.reference_title = None\n",
    "            self.reference_abstract = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:55:31.305819Z",
     "start_time": "2019-08-04T17:55:31.299160Z"
    }
   },
   "outputs": [],
   "source": [
    "#function to clean dirty html of objects that aren't plain text\n",
    "#    or that we don't care about\n",
    "def clean_html(dirty_html_list):\n",
    "    \"\"\"\n",
    "    This function processes lists of dirty_html items into\n",
    "    strings of plaintext clean_html items. This may not be \n",
    "    the most appropriate for things like reference_titles,\n",
    "    but should be okay.\n",
    "    \n",
    "    Args:\n",
    "        dirty_html_list: A `list` of `strings` containing dirty\n",
    "            html of some type (e.g. reference_titles, or body\n",
    "            paragraphs)\n",
    "    \n",
    "    Returns:\n",
    "        clean_html: A `string` containing concatened cleaned\n",
    "            html as given in dirty_html_list\n",
    "    \"\"\"\n",
    "    \n",
    "    #string to concatenate clean html\n",
    "    clean_html=''\n",
    "    \n",
    "    #loop through the dirty items\n",
    "    for dirty_html_item in dirty_html_list:\n",
    "        #loop through objects in dirty items\n",
    "        for obj in dirty_html_item:\n",
    "            \n",
    "            #drop all items unless they are simply formatted text,\n",
    "            #(for these cases: italic, bold, subscript, or \n",
    "            #    superscript)\n",
    "            #If simply formatted, drop the formatting.\n",
    "            #This strategy will result in some ambiguity of \n",
    "            #subscript vs. superscript vs. numbering, but this \n",
    "            #should be better than dropping the formatted text.\n",
    "            if obj.name==None  or obj.name=='i' or obj.name=='b' \\\n",
    "                        or obj.name=='sub' or obj.name=='sup':\n",
    "                clean_html+=obj.string.replace(u'\\xa0', u' ')\n",
    "    return clean_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:55:32.266158Z",
     "start_time": "2019-08-04T17:55:32.259376Z"
    }
   },
   "outputs": [],
   "source": [
    "def html_process(raw_html,journal_family):\n",
    "    \"\"\"\n",
    "    This processes html from an ACS journal to plain text using BeautifulSoup.\n",
    "    All html tags work as of 20190726\n",
    "    \n",
    "    Args:\n",
    "        raw_html: A `string` of the webpage html to be processed\n",
    "        journal_family: A `string` specifying the family of the journal which \n",
    "            indicates the format of the raw_html.\n",
    "        \n",
    "    Returns:\n",
    "        cleaned_html_dict: A `dict` containing\n",
    "            'abstract':  Plaintext `string` of the abstract of the paper\n",
    "            'body': Plaintext `string` of the body of the paper \n",
    "                (including figure captions)\n",
    "            'reference_titles': A `list` containing `strings` of the titles\n",
    "                of each of the references\n",
    "            'reference_abstracts': A `list` containing `strings` of the abstracts\n",
    "                of each of the references\n",
    "    \"\"\"\n",
    "    \n",
    "    ##\n",
    "    #Use BeautifulSoup to and lxml packages to parse html and make it searchable, etc\n",
    "    #the documentation recommended lxml parser as something fast  and lenient\n",
    "    html = BeautifulSoup(html_raw, 'lxml')\n",
    "    ##\n",
    "    \n",
    "\n",
    "    #initialize journal object that contains the html tags and names of relevant\n",
    "    #    parts of the paper we wish to retrieve as attributes\n",
    "    #    (abstracts, body, figure_captions, reference_titles, and reference_abstract)\n",
    "    journal_format_obj = journal_format('acs')\n",
    "    \n",
    "    #loop through attributes of journal_format and obtain cleaned html of elements \n",
    "    #specified by those attributes\n",
    "    cleaned_html_dict = {}\n",
    "    for attr, dict_spec in journal_format_obj.__dict__.items():\n",
    "        #returns a list of raw_html items matching the tag and name specified\n",
    "        #(e.g. abstracts of references or paragraphs)\n",
    "        raw_html_items = html.find_all(dict_spec['tag'],dict_spec['name'])\n",
    "        \n",
    "        #loops through each item, cleans them, and concatenates the result\n",
    "        #(e.g. paragraphs->body or reference_title->big string of all titles\n",
    "        cleaned_html_dict[attr] = clean_html(raw_html_items)\n",
    "\n",
    "    return cleaned_html_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T17:55:34.306350Z",
     "start_time": "2019-08-04T17:55:33.803857Z"
    }
   },
   "outputs": [],
   "source": [
    "print(html_process(html_raw,'acs').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
