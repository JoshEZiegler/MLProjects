2018-12-4:
    The simple linear model doesn't seem to do very well. At first, the best I could do was 54% accuracy - less than if it simply guessed that radiant won every time. I think this makes sense because the features should be highly correlated - no hero alone can predict whether a team will win. I might try some feature engineering but it may work best to move to a DNN model.
    A DNN model seems more natural because heroes belong better in an embedding column (which is a trained layer in a neural network) since there are 116 heroes, but there are simple, lower-dimensional groupings that can be made.
    Before doing this I'm going to try putting all of the heroes into a single feature column and see if that helps. It should?

2018-12-8:
    I was able to get accuracy=58.6 on the test set (accuracy_baseline=55.5%) for a linear_classifier model. Parameters used were: learning_rate=0.03, steps=10000, batch_size=1000. These are a bit aggressive and the steps could probably have been reduced by a factor of ten. In any case, I don’t think that the linear model is going to do any better than this. 
	Across the training period, it didn’t look like the accuracy for the training set decreased much past  60%, but the test set inaccuracy dropped about 1% from ~57.3% to 58.2%. So the extra training time helped the model generalize a bit even if it wasn’t evident from the training accuracy.
	So it looks like the DNN with an indicator_column (one-hot encoding to made a dense vector) is pretty hard to train with the FtrlOptimizer. If the parameters are slightly off the model is prone to either serious overfitting of the training data or underfitting. (hidden_units=[20,10,5])
	Just tried the AdamOptimizer and it worked as good as the FtrlOptimizer with default parameters learning_rate=0.01, hidden_units=[20,10,5]. Not sure why? Here’s some reading on the algorithm (https://arxiv.org/pdf/1412.6980.pdf). It says it’s a gradient descent optimization of stochastic objective functions. Also, “The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.” This algorithm also has a learning rate decay which probs helps reduce noise at late steps.
	In order to do regularization, AdamWOptimizer is nice. It is the same algorithm with a weight decay that regularizes. “It computes the update step of train.AdamOptimizer and additionally decays the variable. Note that this is different from adding L2 regularization on the variables to the loss: it regularizes variables with large gradients more than L2 regularization would, which was shown to yield better training loss and generalization error in the paper above.” I got 58% accuracy with weight_decay=0.001, learning_rate=0.01, hidden_units=[20,10,10,5], steps=10000, batch_size=1000. I’m not sure If I can do better without an embedding layer.
	I was able to get ~58% accuracy with hidden_units=[4,4] which seemed a little better at avoiding overfitting issues. Also weight_decay=0.002, learning_rate=0.01, steps=500, batch_size=1000.Still this isn’t matching the linear model…

2018-12-9:
	Using embedding columns with dimension=2 for the hero inputs easily matches the linear model’s performance. I’m going to look at what the embedding layer is doing to see if it is categorizing heroes in an intuitive way. Then I’ll play around with the hyperparameters and embedding layer dimension. I’d expect that the embedding layer would be better because I can definitely think of >2 independent ways to classify heroes.
	The DNN classifier with an Embedding column, d=4 not really better than d=2, let alone the linear classifier. By plotting the different parts of the embedding layer it looks like the 4th dimension didn’t add much because slices (plotting 4 vs 0 or 2) look like lines, but I also couldn’t really tell the difference between the other ones.
	This project might be a wash because it doesn’t look like the model can do very well at all regardless of complexity. Maybe this is because the data isn’t diverse enough and I should include lower skill level matches. In any case, I want to find a way to get the confusion matrix out as well as look at what the loss is doing as the accuracy gets smaller. If the loss is doing weird stuff it might be a good option to make my own estimator?

2019-2-2:
	After the poor results of the DNN and linear model I got a bit discouraged and just took a bunch of data at various skill levels. I got a total of 7e5 dota2 matches.   This may have reduced the amount of noise while learning? It looks like the training and test set accuracy match v well all the way down to the maximum accuracy. I still was only able to get 58.62% accuracy (compared to a 54.52 baseline accuracy) with the linear model. The agreement between test and training data as well as reproducibility is encouraging. I was v conservative with learning rate and had to do a bunch of steps, but I liked the results. learning_rate=0.002, steps=1e6, batch_size=5000
 
	Linear model, best case Metrics on test set:
		accuracy 0.5862,			
		accuracy_baseline 0.5452214, 
		auc 0.6078432,
		auc_precision_recall 0.6372294, 
		average_loss 0.67102975, 
		label/mean 0.5452214, 
		loss 33.55103, 
		precision 0.59496564, 
		prediction/mean 0.54470664, 
		recall 0.75519633
		global_step 1000000
	Metrics on validation set:
		accuracy 0.5852
		accuracy_baseline 0.5486
		auc 0.6071559
		auc_precision_recall 0.6377059
		average_loss 0.67106634
		label/mean 0.5486
		loss 33.553318
		precision 0.59609306
		prediction/mean 0.5435795
		recall 0.75647104
		global_step 1000000

	My hope is that with this larger amount of data the neural network may be able to find the nonlinear terms more easily. We’ll see if this turns out to be the case!

2019-2-3:	
	LOL TESTS: I used the linear model on the League of Legends data and the accuracy gain over the baseline accuracy was the same as dota2. Baseline: 50.36%, Accuracy: 54.68% with 2e5 games. The model was a bit worse on validation data, but that could just be because it was noisy, baseline accuracy was off by 0.7% compared to the test set. It looked like with more steps it was possible to push it a few hundredths of a percent higher, but this took a couple hours so its fine enough. learning_rate=0.003, steps=5e5, batch_size=2000

	Linear model, best case metrics on test set:
		accuracy 0.54675937
		accuracy_baseline 0.5036985
		auc 0.5667639
		auc_precision_recall 0.56329566
		average_loss 0.6861083
		label/mean 0.5036985
		loss 34.29707
		precision 0.5493952
		prediction/mean 0.5016105
		recall 0.55709404
		global_step 500000
	Validation set:
		accuracy 0.543
		accuracy_baseline 0.5102
		auc 0.56560886
		auc_precision_recall 0.5488617
		average_loss 0.68615097
		label/mean 0.4898
		loss 34.30755
		precision 0.5322327
		prediction/mean 0.50139517
		recall 0.55287874
		global_step 500000

2019-3-23:
	DOTA2 TESTS: With the DNN and an embedding layer I was able to get 59.12% accuracy. This is a gain of 0.5% over the linear model, which doesn’t sound like much, but it is 10% increased accuracy gain beyond baseline. The fact that this gain isn’t reflected in the validation set accuracy is a bit concerning though. I’m not sure how this might be remedied. It could be that I am overfitting the test set somehow, but I’m not sure about this (learning_rate = 0.002, weight_decay=.0002, embedding_dimension=3, hidden_units=[8,4,4,4], steps=4e4)
	DNN model, best case metric on test set:
		accuracy 0.59121925
		accuracy_baseline 0.5452414
		auc 0.61524504
		auc_precision_recall 0.6447893
		average_loss 0.6684269
		label/mean 0.5452414
		loss 1994.1302
		precision 0.60509056
		prediction/mean 0.54183614
		recall 0.7205189
		global_step 50000
	Validation set:
		accuracy 0.5869868
		accuracy_baseline 0.5434724
		auc 0.61382484
		auc_precision_recall 0.6421689
		average_loss 0.66916025
		label/mean 0.5434724
		loss 33.45801
		precision 0.60038733
		prediction/mean 0.5420707
		recall 0.7178271
		global_step 50000

2019-4-1:
	DOTA2 TESTS: Did a bit better on both the test and validation with a bit more tuning. I’m not sure if this is how the variable and layers work, but with hidden_units=[10,10] the dnn learned p good and that does correspond on the total number of heroes… I was able to save this trained model to the ML dota2 directory so I can apply some test cases and see what the model thinks! I want to try to find some bad drafts and see what the computer thinks (weight_decay=4e-4, learning_rate=2.5e-3, hidden_units=[10,10], steps=3e4, batch_size=3e3)
MODEL SAVED
	Test set:
		accuracy 0.59158844
		accuracy_baseline 0.5452399
		auc 0.6168599
		auc_precision_recall 0.64596987
		average_loss 0.6678934
		label/mean 0.5452399
		loss 1992.6101
		precision 0.5989614
		prediction/mean 0.5533037
		recall 0.7594368
		global_step 30000
	Validation set:
		accuracy 0.5890347
		accuracy_baseline 0.543492
		auc 0.6158235
		auc_precision_recall 0.64411813
		average_loss 0.6686505
		label/mean 0.543492
		loss 33.432526
		precision 0.59568346
		prediction/mean 0.5534632
		recall 0.75902987
		global_step 30000

2019-4-13:
	LOL TESTS: I was able to get almost 55% accuracy (4.7% gain over baseline accuracy) with a DNN model the gain is still p slight over the linear model. I save this model so I can do some test cases once I figure out how to do serving.(weight_decay=0.0008, learning_rate=0.0025, hidden_units=[10,10], steps=3e3, batch_size=3000) MODEL SAVED
	Test set metrics:
		accuracy 0.54958797
		accuracy_baseline 0.50366473
		auc 0.568422
		auc_precision_recall 0.5605323
		average_loss 0.68605167
		label/mean 0.50366473
		loss 1978.1339
		precision 0.5490022
		prediction/mean 0.5075455
		recall 0.5922823
		global_step 3000
	Validation set metrics:
		accuracy 0.5495492
		accuracy_baseline 0.50042677
		auc 0.5685406
		auc_precision_recall 0.5591297
		average_loss 0.6860394
		label/mean 0.50042677
		loss 34.295567
		precision 0.5458012
		prediction/mean 0.507441
		recall 0.5950433
		global_step 3000

2019-05-06:
	LOL Gold_diff tests: Was only able to get to loss=0.977 for the linear model with just the scaled gold_diff as the target. I believe base loss=1 so this is a gain of 2.3%, sort of comparable with the prediction power of the DNN/linear model with blue_win as the sole target. Despite this, it may be easier to train gold and win together than alone?

	Test set metrics:
		average_loss 0.97686446
		label/mean 0.0003589372
		loss 0.97685313
		prediction/mean 0.0021375013
		global_step 150000
		---
	Validation set metrics:
		average_loss 0.962123
		label/mean 0.010293012
		loss 0.96212316
		prediction/mean 0.0026842875
		global_step 150000

2019-06-09:
	LOL single target learning on new dataset (3e5 games). The loss was a bit higher on these, but I’m going to go ahead and go for it. The increase in loss was only 0.5, but that is about 1/4 of the gain over the base loss so it is fairly significant. I can just use old data if I’m concerned about getting a lower loss, but that isn’t really the point here so I’m going to start playing with the dual_head model.

	Blue_win:
	Test set metrics:
		accuracy 0.53044343
		accuracy_baseline 0.5049286
		auc 0.5452918
		auc_precision_recall 0.5311785
		average_loss 0.69011754
		label/mean 0.49507144
		loss 0.69011706
		precision 0.5305559
		prediction/mean 0.49344492
		recall 0.44743603
		global_step 50000
	gold_diff:
	Test set metrics:
		average_loss 0.98316014
		label/mean 0.00010354905
		loss 0.98316056
		prediction/mean -0.00010493766
		global_step 500000

2019-06-23:
	LOL dualhead_GoldWin testing: The first-order dual head model has gotten about as good accuracy as the single target win model. I used the same hidden layers for both targets, with only the linear output weights different (known as hard parameter sharing). The loss function is a linear combination of the mean square error for each target. I calibrated it to be equal contribution from each pre-training. This accuracy was difficult to obtain due to a local minimum in the loss. Parameters used for these results were AdamWOptimizer, learning_rate=3e-6, weight_decay=3e-5, steps=1e6, batch_size=1000, hidden_units=[8,8].
	Test set metrics:
		accuracy 0.53365046
		auc 0.5324331
		baseline_accuracy 0.5
		loss 0.30886853
		global_step 1000000

2019-07-13:
	Dota2 radiant_win testing: The linear model was able to get 58.59% accuracy (compared to 55.22% baseline) with learning_rate=5e-4, steps=1e3, batch_size=5e4 using AdamW. Fixed the corrupted record stuff that made it a pain to get play with hyperparams more.
	validation set metrics:
		accuracy 0.5859396
		accuracy_baseline 0.55225146
		auc 0.60252357
		auc_precision_recall 0.6390419
		average_loss 0.6715866
		label/mean 0.55225146
		loss 3222.7559
		precision 0.59855545
		prediction/mean 0.5471199
		recall 0.75986564
		global_step 1000

2019-07-16:
	Dota2 radiant_win testing: The DNN did a bit worse than the linear model (probably could match with more testing) at 58.46% (Compared to 55.19% baseline)
	validation set metrics:
		accuracy 0.5845934
		accuracy_baseline 0.551943
		auc 0.60160893
		auc_precision_recall 0.6373349
		average_loss 0.6722902
		label/mean 0.551943
		loss 3226.1326
		precision 0.60186946
		prediction/mean 0.5446683
		recall 0.73077303
		global_step 1000